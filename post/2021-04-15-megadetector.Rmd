---
title: "Using Microsoft's MegaDetector to Detect Animals on Cameratrap Images"
summary: "In this blog-post, I'm going to illustrate how you can use Microsoft's MegaDetector to automatically detect animals in camera trap images. This can be very useful to automate the identification and removal of images that do not contain any animals."
author: "David D. Hofmann"
date: 2021-04-15T18:00:00-05:00
categories: ["R"]
tags: ["R", "deep learning", "cameratrap", "megadetector"]
image: "img/post_megadetector.jpg"
output:
  blogdown::html_page:
    highlight: tango
    toc: true
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

# Introduction
Wildlife cameras (Figure 1) have become an increasingly popular tool to monitor
and study animal populations. They are non-invasive and, once set up, can be
left in the field for months with only little maintenance. Thereby, the allow to
laregly automate the collection of images that grant exciting insights into
wildlife. On the downside, wildlife cameras often produce thousands of images
that all need to be screened by trained biologists. This can be very
time-consuiming and cumbersome, particularly if there are many false positives,
i.e. images on which there are actually no animals.

```{r, echo=F, messages=F, out.width="50%", fig.cap="A Brownings wildlife camera that I've set up in my garden to learn about our nocturnal visitors from the nearby forest."}
knitr::include_graphics("/img/gallery_megadetector/figure1.jpg", error = F)
```

With the uprise of deep learning techniques, such tasks can finally be tackled
using computer vision. For cameratrap images in particular, Microsoft has
recently released a neural network called the "MegaDetector". The MegaDetector
is a powerful neural network that has been trained to distinguish the categories
"humans", "vehicles", and "animals" on any image. To learn about those
categories, it has been exposed to millions of cameratrap images with known
detections and known absences. Even though the detector only detects three
categories, it does so with great reliability. Consequently, the MegaDetector
may serve as a first filter to remove empty images and to reduce the amount of
images that need to be visually inspected. In this blog-post, I want to outline
a general workflow that allows you to apply the MegaDetector on your own set of
images and to process the resulting output to produce images with bounding boxes
around detections.

# Installation
Probably the most difficult part of using the MegaDetector is to get everything
up and running. I will therefore try to be very thorough and explain each step
in as much detail as I can. Nevertheless, I will have to refer you to other
sources in some cases. Much of this blog post has been inspired by [Microsoft's
Github
Instructions](https://github.com/microsoft/CameraTraps/blob/master/megadetector.md#using-the-models)
which contains all the basic information on using the *MegaDetector*.

## Installing Python
To use the MegaDetector, you need to have python installed. Even if your
computer comes with a system installation of python, I would highly recommend
that you install python using
[conda](https://docs.conda.io/en/latest/miniconda.html). Conda is an accessible
package and environment manager that allows you to easily create virtual
environments containing python and packages of a desired version. This
drastically reduces the need to manually fiddle with different versions. The
installation conda differs depending on your operating system and is explained
in detail here:

* [Windows](https://conda.io/projects/conda/en/latest/user-guide/install/windows.html)
* [MacOS](https://conda.io/projects/conda/en/latest/user-guide/install/macos.html)
* [Linux](https://conda.io/projects/conda/en/latest/user-guide/install/linux.html#)

Once you have conda installed, open up a new terminal. If conda installed
correctly, you should see an indicator telling you that conda booted into the
*base* environment.

```{bash, eval = FALSE}
# In brackets, conda always tells you in which environment you are working
(base) david@minty:~$
```

Using conda, it is very easy to create a new python environment. In fact, we are
going to create a new environment called *cameratrapping* that uses python
version 3.6 right now. To do so, we run the following command:

```{bash, eval = FALSE}
# Create new virtual environment that runs python version 3.6
conda create -n cameratrapping python=3.6
```

Note that we specifically tell conda to use python version 3.6. This is extremely
important as some of the packages we'll be using do not work with more up to
date versions of python. Once the setup of the new environment is complete, we
still need to switch into it. This can be done by "activating" the new
environemnt:

```{bash, eval = FALSE}
# Switch to the new environment
conda activate cameratrapping
```

After activation, you're terminal should indicate that conda switched to the
*cameratrapping* environment.

```{bash, eval = FALSE}
# Conda should have switched to the "cameratrapping" environment now
(cameratrapping) david@minty:~$
```

With `activate environment-name` you can always go back and forth between
different environments. To get a list of all environments, simply call:

```{bash, eval = FALSE}
# Show all available environments
conda env list
```

## Installing Tensorflow and Other Python Packages
To be able to run the *MegaDetector*, we'll need to install tensorflow version
1.13.1. This is a rather old version and also the reason why we had to opt for
python version 3.6. To install it, make sure that you are working in the
*cameratrapping* environment and then use *pip* to install tensorflow version
1.13.1:

```{bash, eval = FALSE}
# Install tensorflow
pip install tensorflow==1.13.1
```

In addition to tensorflow, there are several other python packages we need to
install. However, the exact versions do not matter here. We can thus install all
of them as follows:

```{bash, eval = FALSE}
# Install other dependencies
pip install pillow humanfriendly matplotlib tqdm jsonpickle statistics requests
```

## Downloading Additional Files from Github
In addition to the above installed python packages, we also need to download
some folders and files that provide further functionalities. I would recommend
that you download the files into a distinct folder called *CameraTrapping*. If
you are familiar with git and have it installed on your computer, you can easily
download the required files using git (see Option 1). Oterhwise, you can also
download them manually (see Option 2).

### Option 1: Using Git
```{bash, eval = FALSE}
# Change directory (cd) into the "CameraTrapping" folder. The exact path will be
# different for you!
cd /home/david/Schreibtisch/CameraTrapping

# Download required files and folders from GitHub
git clone https://github.com/Microsoft/cameratraps
git clone https://github.com/Microsoft/ai4eutils
wget https://raw.githubusercontent.com/microsoft/CameraTraps/master/detection/run_tf_detector.py
wget https://raw.githubusercontent.com/microsoft/CameraTraps/master/detection/megadetector.md#2-run_tf_detector_batchpy.py
wget https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb

# OPTIONAL: Download example images
git clone https://github.com/DavidDHofmann/sample_images

```

### Option 2: Manual Download
You can manually download the files from the links below. Store all of the files
and folders into a distinct folder called *CameraTrapping* and make sure their
names are similar to those highlighted in blue.

  - [cameratraps](https://github.com/Microsoft/cameratraps) (Github Repository / Folder)
  
  - [ai4eutils](https://github.com/Microsoft/ai4eutils) (Github Repository / Folder)
  
  - [run_tf_detector.py](https://github.com/Microsoft/CameraTraps/blob/master/detection/run_tf_detector.py) (Python Script)
  
  - [run_tf_detector_batch.py](https://github.com/microsoft/CameraTraps/blob/master/megadetector.md#2-run_tf_detector_batchpy) (Python Script)
  
  - [md_v4.1.0.pb](https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb) (Model File)
  
  - [sample_images](https://github.com/DavidDHofmann/sample_images) (Some sample camertrap images that I recorded in my garden)
  
## Making the Downloaded Files Available to Python
We need to make sure that the downloaded folders "ai4eutils" and "cameratraps"
can be found by python. To ensure this, we can run the commands below. Note that
you will need to execute this code everytime you start a new session.

```{bash, eval = FALSE}
# Make folders available to python
export PYTHONPATH="$PYTHONPATH:$PWD/ai4eutils:$PWD/cameratraps"
```

## A Brief Overview of the Downloaded Files
If everything worked fine, your *CameraTrapping* folder should have the
following structure.

```
CameraTrapping
└───ai4eutils
    └───...
└───cameratraps
    └───...
└───sample_images
    │ L002_R01_20210307212248.JPG
    │ L002_R01_20210308022921.JPG
    │ ...
    └───detections
│   md_v4.1.0.pb
│   run_tf_detector.py   
│   run_tf_detector_batch.py
```

Let me briefly explain each of the elements are used for. We don't really need
to worry about the first two folders (*ai4utils* and *cameratraps*), as they
simply provide some tools that the *MegaDetector* needs. The folder
*sample_images* contains several example images from camera traps that we can
use to test the *MegaDetector*. The *md_v4.1.0.pb* file contains the
MegaDetector and is the "brain" of the detection algorithm. The two python
scripts, on the other hand, serve as interface and allow us to initiate the
*MegaDetector*. The first script (*run_tf_detector.py*) is intended for testing
only and serves to quickly visualize detections on images with a nice bounding
box. The second script (*run_tf_detector_batch.py*) runs detections in batches
and periodically saves the results. While it does not create the neat-looking
bounding box, it outputs a .json file that indicates all detections. This is
usually the script you'll use when you want to make detections on a large batch
of images. Finally, the .pb file is our pre-trained neural network. It is like
the brain of the operation and does all the hard work for us!

# Running the MegaDetector
As stated above, there are two different ways in which we can run the
MegaDetector:

* Approach No. 1: We use the *run_tf_detector.py* script, which automatically
runs the detection algorithm and returns one or multiple images on which
bounding boxes around all detections are drawn. This approach is more suitable
for testing and for small sets of images if you only want to quickly visualize
some detections. However, for more scientific purposes it is not very useful
because the detection information is not stored in a structured manner and can't
be accessed or used for further analysis.

* Approach No. 2: We use the *run_tf_detector_batch.py* script which runs the
detection algorithm and returns a "list of lists" containing all detections,
including a measure of confidence. While this approach does not directly produce
images with neat bounding boxes, it is much more useful for scientific purposes.
In addition, this script periodically saves the output and is therefore more
suitable if you're running the detection on a large set of images.

In either case, you should now double-check that everything is setup correctly.
Make sure that your terminal points to the correct directory, i.e. that you're
working within the *Cameratrapping* folder and that you are using the
*cameratrapping* conda environment.

## Approach I
In this approach, we use the *run_tf_detector.py* script. The script can be
applied to a single image or on an entire directory. The commands to invoke the
script are the following (you'll need to fill in *filename* and *directoryname*
by yourself):

```{bash, eval = FALSE}
# If you want to run the detection on a single image
python run_tf_detector.py md_v4.1.0.pb --input_file filename --output_dir directoryname

# If you want to run the detection on an entire directory
python run_tf_detector.py md_v4.1.0.pb --input_dir directoryname --output_dir directoryname
```

* `python run_tf_detector.py` tells our computer to run the run_tf_detector.py
  script using python.

* `md_v4.1.0.pb` is our model file (i.e. the pre-trained neural
  network).

* `--input_file filename` allows us to specify the image on which we want to
  detect images

* `--input_dir directoryname` allows us to specify the directory containing all
  images on which we want to detect animals
  
* `--output_dir directoryname` allows us to specify the directory into which the
  newly created images should be stored
  
There are several additional options you may want to play with. To get the full
list of available options, simply type:

```{bash, eval = FALSE}
# Show available options
python run_tf_detector.py -h
```

Anyways, let's run the detector on some real images. In case you have downloaded
my example images, the commands above translate into:

```{bash, eval = FALSE}
# Run MegaDetector on a single image
python run_tf_detector.py md_v4.1.0.pb --image_file sample_images/L002_R01_20210314013742.JPG --output_dir sample_images/detections

# Run MegaDetector on an entire directory
python run_tf_detector.py md_v4.1.0.pb --image_dir sample_images --output_dir sample_images/detections
```

If everything is setup correctly, the *MegaDetector* will get to work and run
the detections. Once the algorithm terminates, you can go to the
*sample_images/detections* folder where you should find the final images with
bounding boxes around detected animals. The images should look something like
shown in Figure 3b).

```{r, echo=F, messages=F, out.width="100%", fig.cap='a) Input image and b) output image produced by the run_tf_detector.py script.'}
knitr::include_graphics("/img/gallery_megadetector/figure2.png", error = F)
```

## Approach II
In this approach, we use the *run_tf_batch_detector.py* script. While this
script does not generate neat looking pictures with bounding boxes, it creates a
*.json* file that contains all the detections in a structured list. This allows
us to import the file into R for further analysis. To invoke the batch-detector
script, we can use the following command (you'll need to fill in
*filename/directory* and *filename.json* by yourself):

```{bash, eval = FALSE}
# To run the MegaDetector batch file
python run_tf_detector_batch.py md_v4.1.0.pb --input_file filename/directory --output_file filename.json
```

* `python run_tf_detector_batch.py` tells our computer to run the
  run_tf_detector_batch.py script using python.

* `md_v4.1.0.pb` is the *MegaDetector* model file (i.e. the pre-trained neural
  network).

* `--input_file filename/directory` allows us to specify the image or a
  directory for which we'd like to run the algorithm

* `--output_file filename.json` allows us to specify the directory into which
  the .json file shoud be stored

For our sample images, this code snippet translates into:

```{bash, eval = FALSE}
# Run the MegaDetector and create a .json file
python run_tf_detector_batch.py md_v4.1.0.pb /home/david/Schreibtisch/CameraTrapping/sample_images /home/david/Schreibtisch/CameraTrapping/sample_images/detections/detections.json
```

# Processing the MegaDetector Output
In case you run the *run_tf_detector_batch.py*, you'll end up with a .json file
containing all detections. This file needs to be further processed. Here, I'm
going to use R to import and clean the file and to visualize some of the
detections.

```{r, warning=F, message=F}
# Load required packages
library(rjson)      # To read a json file
library(tidyverse)  # For data wrangling
library(exifr)      # To read image meta-data
library(magick)     # To plot images
library(lubridate)  # To handle timestamps

# Set working directory (this will be different for you)
setwd("/home/david/Schreibtisch/CameraTrapping/sample_images")

# Load json file produced by the MegaDetector
anot <- fromJSON(file = "detections/detections.json")

```

Let's take a look at the structure of the .json file.

```{r, warning=F, message=F}
# Look at the structure of the json file
str(anot, 2)
```

The .json file is basically a list of lists containing lots of information on
the input images, detected categories, and on the algorithm itself. We will
consolidate all of this information into a single tibble/dataframe that is more
visually appealing:

```{r, warning=F, message=F}

# Extract detected categories and put them into a tibble/dataframe
cats              <- enframe(anot$detection_categories)
names(cats)       <- c("CategoryID", "CategoryName")
cats$CategoryName <- unlist(cats$CategoryName)

# Get information into nice format
info <- tibble(
      images     = map_chr(anot$images, "file")
    , detections = lapply(anot$images, function(x){
      tibble(
          category   = map_chr(x$detections, "category")
        , confidence = map_dbl(x$detections, "conf")
        , bbox       = map(x$detections, "bbox")
      )
    })
  ) %>%
  unnest(detections) %>%
  mutate(bbox = map(bbox, function(x){paste0(x, collapse = ", ")})) %>%
  separate(bbox
    , sep     = ","
    , into    = c("x_left", "y_top", "x_width", "y_height")
    , convert = T
  )

# Join categories to the info table
info <- left_join(info, cats, by = c("category" = "CategoryID"))

# Look at the generated dataframe
head(info)
```

Take a look at the output from the cleaned dataframe. It contains a row for each
detection. Thus, if one image contains more than one detection, there will be
multiple rows for the same image. By looking at the columns, we see that the
file contains information on the detected categories (*cateogry* &
*CategoryName*) and on the bounding boxes surrounding each of the detections.
The bounding boxes are defined by an x and y coordinate (*x_left* & *y_top*) in
conjunction with a width and a height (*x_width* & *y_height*). The respective
numbers are given on a scale from 0 to 1, so that we'll need to adjust the
bounding box depending on the image resolution. It is also important to note
that in computer vision the y axis starts at the top and not from the bottom. We
also see that for each detection the detector returned a measure of confidence
(*confidence*).

We can now use this table to conduct a wide range of exploratory analyses. For
instance, we may be interested in all images on which the detector identified
animals with a relatively high confidence. We can easily identify those images
as follows:

```{r}
# Identify images that contain animals
info %>%
  subset(CategoryName == "animal" & confidence > 0.9)

```

By default, the MegaDetector also returns detections for which he is very
uncertain.

```{r, out.width = "50%", fig.caption = "Histogram of the confience with which the MegaDetector identified categories in our sample images."}
# Histogram of MegaDetectors confidence
hist(info$confidence
  , breaks = 10
  , main   = "Histogram of Confidence"
  , xlab   = "Confidence"
  , ylab   = "Frequency"
  , col    = "cornflowerblue"
  , border = NA
)

```

For now, however, we only care about relatively certain detections, so I'll
subset the data accordingly.

```{r}
# Subset to detections with high confidence
info <- info %>%
  subset(confidence > 0.9)

```

You may also wonder during which time of the day you detected most animals. For
this, we need to extract the timestamp of each cameratrap image. We can use the
`exifr` package to extract such information from the image metadata.

```{r}
# Load image metadata
meta <- read_exif(info$images)

# This contains a ton of information!
names(meta)

# Extract timestamp and add it to our "info" table
info$Timestamp      <- as.POSIXct(meta$CreateDate, format = "%Y:%m:%d %H:%M:%S")
info$TimestampRound <- round(info$Timestamp, "hour")

# Identify number of detections depending on the time of the day
info %>%
  subset(CategoryName == "animal") %>%
  mutate(TimeOfDay = ifelse(hour(TimestampRound) %in% 6:22, "Day", "Night")) %>%
  count(TimeOfDay)

```

In this case, only one of the eight animals appeared during the day. Most often,
you will want to visualize the detections. For this, we'll need to load the
respective image into R, draw a bounding box around the detected object and plot
everything. We can write a function that allows us to automate these tasks:

```{r, warning=F, message=F, out.width="100%", fig.cap="Some example detections on cameratrap images. Differently colored bounding boxes represent the different categories. The MegaDetector is able to distinguish animals, humans (person), and vehicles. As you can see on the image on the lower right, the detector even detects animals that are not fully within the picture!"}
# Function to plot image with bounding box around the detection
showDetection <- function(i = NULL){
  
  # Load ith image
  img  <- image_read(info$images[i])
  dims <- image_info(img)
  
  # Plot
  img <- image_draw(img)
  
  # Add bounding box around the detection (color depending on detected category)
  rect(
      xleft   = info$x_left[i] * dims$width
    , xright  = (info$x_left[i] + info$x_width[i]) * dims$width
    , ytop    = info$y_top[i] * dims$height
    , ybottom = (info$y_top[i] + info$y_height[i]) * dims$height
    , border  = c("red", "yellow", "green")[as.numeric(info$category[i])]
    , lwd     = 10
  )
  
  # Add a text label indicating the detected class (note that we ensure that the
  # label does not leave the image border on the y-axis).
  text(
      x      = info$x_left[i] * dims$width
    , y      = max(info$y_top[i], 0.05) * dims$height
    , labels = paste(info$CategoryName[i], info$confidence[i])
    , cex    = 5
    , col    = c("red", "yellow", "green")[as.numeric(info$category[i])]
    , adj    = c(0, 0)
  )
  
  # Add text indicating the image name
  text(
      0.5 * dims$width
    , 0.95 * dims$height
    , basename(info$images[i])
    , cex = 5
    , col = "white"
    , pos = 3
  )
  dev.off()
  plot(img)
  
}

# Let's try it on some example images
par(mfrow = c(3, 2), mar = c(0, 0, 0, 0))
plots <- lapply(1:6, function(i){showDetection(i)})
  
```

Obviously the *MegaDetector* is not perfect and there will always be some
misclassifications. Nevertheless, I am often very impressed by the
MegaDetector's ability to identify animals, even on low-light black and white
images. It has happened to me multiple times that I skimmed through images and
didn't realize that an animal was lurking somewhere in the background, yet the
MegaDetector picked it up perfectly!

# Summary
In this blog post, we have explored how we can use the *MegaDetector* to easily
identify animals on cameratrap images. We have also learned how we can import
the .json output file into R to produce neat-looking images with bounding boxes
around each detected animal. Overall, the *MegaDetector* is a very powerful and
fun tool that allows us to automatically detect animals on cameratrap images.

# Session Information
```{r, echo=F}
sessionInfo()
```

