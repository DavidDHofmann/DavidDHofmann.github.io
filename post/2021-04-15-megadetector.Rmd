---
title: "Using Microsoft's MegaDetector to Detect Animals on Cameratrap Images"
summary: "Camera trap studies usually deal with thousands of camera trap images that have to be organized and sorted. In this blog-post, I'm going to illustrate how you can use Microsoft's MegaDetector to automatically detect animals in camera trap images. This can be very useful to automate the identification and removal of images that do not contain any animals."
author: "David D. Hofmann"
date: 2021-04-15T18:00:00-05:00
categories: ["R"]
tags: ["R", "deep learning", "cameratrap", "megadetector"]
image: "img/post_megadetector.jpg"
output:
  blogdown::html_page:
    highlight: tango
    toc: true
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

# Introduction
Wildlife cameras (Figure 1) have become an increasingly popular tool to monitor
and study animal populations. One of the main reason why cameras have become so
omnipresent in ecology is because they are non-invasive and, once set up, can be
left in the field for months with only little maintenance. This allows for a
laregly automated collection of images that grant exciting insights into
wildlife. On the downside, wildlife cameras often produce thousands of images
that all need to be screened by trained biologists to identify the animals on
each image. This can be very time-consuiming and cumbersome, particularly if
many of the images are empty and do not contain any animals (so called false
positives). Consequently, there has been growing interests towards automating
these tedious processes with the aid of computers.

```{r, echo=F, messages=F, out.width="50%", fig.cap="A Brownings wildlife camera that I've set up in my garden to learn about our nocturnal visitors from the nearby forest."}
knitr::include_graphics("/img/gallery_megadetector/figure1.jpg", error = F)
```

With the uprise of deep learning techniques, such tasks can now finally be
tackled efficiently using computer vision. For cameratrap images in particular,
Microsoft has recently released a neural network called the "MegaDetector". The
MegaDetector is a powerful neural network that has been trained to distinguish
the categories "humans", "vehicles", and "animals" on any kind of image. To
learn about those categories, it has been exposed to millions of images with
known detections and known absences of those categories. Ultimately, the
computer itself learned how to detect humans, vehicles, and animals on images.
Even though the detector only detects these three categories, it does so with
great reliability. Consequently, the MegaDetector may serve as a first filter to
remove empty images, thereby reducing the amount of images that need to be
visually inspected by a human being. In this blog-post, I will to outline a
simple workflow that allows you to apply the MegaDetector on your own set of
images. Finally, we will make use of the output file generated by the
MegaDetector to visualize a set of images with bounding boxes around the
different detections in each image.

# Installation
Probably the most difficult part of using the MegaDetector is to set everything
up and running. Hence, I will try to be as thorough as I can and explain each
step in as much detail as possible. Nevertheless, I may refer to other sources
in some cases. Also note that much of this blog post has been inspired by
[Microsoft's Github
Instructions](https://github.com/microsoft/CameraTraps/blob/master/megadetector.md#using-the-models)
which contains all the basic information on using the *MegaDetector*.

## Installing Python
In order to use the MegaDetector, you need to have python installed on your
computer. Some computers already have a system installation of python, yet I
would still highly recommend that you install python using
[conda](https://docs.conda.io/en/latest/miniconda.html). Conda is a really
accessible package and environment manager that allows you to easily create
virtual environments containing python and packages of a desired version. This
drastically reduces the need to manually fiddle with different versions of
different packages and enables you to test your nasty python scripts in a save
environment. Depending on your operating system the installation of conda
differs, hence I suggest that you check out the specific installation
instructions here:

* [Windows](https://conda.io/projects/conda/en/latest/user-guide/install/windows.html)
* [MacOS](https://conda.io/projects/conda/en/latest/user-guide/install/macos.html)
* [Linux](https://conda.io/projects/conda/en/latest/user-guide/install/linux.html#)

Once you have installed conda, open up a new terminal. If conda installed
correctly, you should now see an indicator telling you that conda booted into
the *base* environment.

```{bash, eval = FALSE}
# In brackets, conda always tells you in which environment you are working
(base) david@minty:~$
```

With conda, it is now very easy to create a new python environment. In fact, we
are going to create a new environment called *cameratrapping* that uses python
version 3.6. To do so, we run the following command:

```{bash, eval = FALSE}
# Create new virtual environment that runs python version 3.6
conda create -n cameratrapping python=3.6
```

Note that we specifically tell conda to use python version 3.6. This is
extremely important as some of the packages we'll be using do not work with more
up to date versions of python. However, thanks to conda it is rather easy to
install different python versions. Anyways, once you successfully setup the new
environment, you still need to switch into it. This can be done by "activating"
the new environemnt:

```{bash, eval = FALSE}
# Switch to the new environment
conda activate cameratrapping
```

After activation, you're terminal should indicate that conda switched to the
*cameratrapping* environment, meaning that you left the base environment and are
now working in the cameratrapping environment (which runs python version 3.6).

```{bash, eval = FALSE}
# Conda should have switched to the "cameratrapping" environment now
(cameratrapping) david@minty:~$
```

With `activate environment-name` you can always go back and forth between
different environments depending on your needs. To get a list of all
environments, simply call:

```{bash, eval = FALSE}
# Show all available environments
conda env list
```

## Installing Tensorflow and Other Python Packages
To be able to run the *MegaDetector*, we'll need to install tensorflow version
1.13.1. This is a rather old version and also the reason why we had to opt for
python version 3.6. To install the specific version of tensorflow, first make
sure that you are working in the *cameratrapping* environment, then use *pip*
to install tensorflow version 1.13.1:

```{bash, eval = FALSE}
# Install tensorflow
pip install tensorflow==1.13.1
```

In case your computer has a dedicated gpu, you may also run the megadetector on
the gpu. In this case, you would need to install tensorflow slightly
differently:

```{bash, eval = FALSE}
# Install tensorflow-gpu (using conda instead of pip)
conda install -c anaconda tensorflow-gpu=1.13.1
```

Besides tensorflow, we also need to install several other python packages.
However, here the exact versions do not really matter and we can thus install
all of them as follows:

```{bash, eval = FALSE}
# Install other dependencies
pip install pillow humanfriendly matplotlib tqdm jsonpickle statistics requests
```

## Downloading Additional Files from Github
In addition to the above installed python packages, we also need to download
some folders and files that provide further functionalities. I would recommend
that you download the files into a distinct folder called *CameraTrapping*. If
you are familiar with git and have it installed on your computer, you can
download the required files using git (see Option 1). Oterhwise, just download
them manually (see Option 2).

### Option 1: Using Git
```{bash, eval = FALSE}
# Change directory (cd) into the "CameraTrapping" folder. The exact path will be
# different for you!
cd /home/david/Schreibtisch/CameraTrapping

# Download required files and folders from GitHub
git clone https://github.com/Microsoft/cameratraps
git clone https://github.com/Microsoft/ai4eutils
wget https://raw.githubusercontent.com/microsoft/CameraTraps/master/detection/run_tf_detector.py
wget https://raw.githubusercontent.com/microsoft/CameraTraps/master/detection/run_tf_detector_batch.py
wget https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb

# OPTIONAL: Download example images
git clone https://github.com/DavidDHofmann/sample_images

```

### Option 2: Manual Download
You can manually download the files from the links below. Store all of the files
and folders into a distinct folder called *CameraTrapping* and make sure their
names are similar to those highlighted in blue.

  - [cameratraps](https://github.com/Microsoft/cameratraps) (Github Repository / Folder)

  - [ai4eutils](https://github.com/Microsoft/ai4eutils) (Github Repository / Folder)

  - [run_tf_detector.py](https://github.com/Microsoft/CameraTraps/blob/master/detection/run_tf_detector.py) (Python Script)

  - [run_tf_detector_batch.py](https://github.com/microsoft/CameraTraps/blob/master/run_tf_detector_batch.py) (Python Script)

  - [md_v4.1.0.pb](https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb) (Model File)

  - [sample_images](https://github.com/DavidDHofmann/sample_images) (Some sample camertrap images that I recorded in my garden)

## Making the Downloaded Files Available to Python
Finally, we need to make sure that the downloaded folders "ai4eutils" and
"cameratraps" can be found by python. To ensure this, we can run the commands
below. Note that you will need to execute this code everytime you start a new
session in the respective environemnt.

```{bash, eval = FALSE}
# Make folders available to python
export PYTHONPATH="$PYTHONPATH:$PWD/ai4eutils:$PWD/cameratraps"
```

## A Brief Overview of the Downloaded Files
If everything worked fine, your *CameraTrapping* folder should now have the
following structure.

```
CameraTrapping
└───ai4eutils
    └───...
└───cameratraps
    └───...
└───sample_images
    │ L002_R01_20210307212248.JPG
    │ L002_R01_20210308022921.JPG
    │ ...
    └───detections
│   md_v4.1.0.pb
│   run_tf_detector.py
│   run_tf_detector_batch.py
```

Let me briefly explain what each of these elements are used for. The first two
elements (*ai4utils* and *cameratraps*) are simply two folders that contain some
tools that the *MegaDetector* needs. We don't need to worry about those much.
The folder *sample_images* contains several example images from camera traps
that I collected in my (girlfriend's) garden. We can use those to test if the
*MegaDetector* actually works. The *md_v4.1.0.pb* file contains the
MegaDetector. You can think of this as the "brain" of our little operation and
is the "brain" of the detection algorithm. Furthermore, there are two python
scripts (.py). These serve as interface and allow us to initiate the
*MegaDetector* through the terminal/command line. The first script
(*run_tf_detector.py*) is intended for testing only and serves to quickly
generate and visualize some detections. It basically allows us to take an input
image and to return another image highlighting the detected objects. The second
script (*run_tf_detector_batch.py*) runs detections in batches and periodically
saves the results. While it does not create neat looking images showing the
different detections, it outputs a .json file that lists all detections in a
structured way. This is usually the script you'll use when you want to make
detections on a large batch of images.

# Running the MegaDetector
As stated above, there are two different ways in which we can run the
MegaDetector:

* Approach No. 1: We use the *run_tf_detector.py* script, which automatically
runs the detection algorithm and returns one or multiple images on which
bounding boxes around all detections are drawn. This approach is more suitable
for testing and for small sets of images if you only want to quickly visualize
some detections. However, for more scientific purposes it is not very useful
because the detection information is not stored in a structured manner and can't
be accessed or used for further analysis.

* Approach No. 2: We use the *run_tf_detector_batch.py* script which runs the
detection algorithm and returns a "list of lists" containing all detections.
Conveniently, each detection has an associated uncertainty, showing us how
"confident" the detector is for that specific detection. While this approach
does not directly produce images with neat bounding boxes, it is much more
useful for scientific purposes, as we can easily use the returned file for
further analysis and subsetting of the different images. In addition, this
script periodically saves the MegaDetector's so that not all is lost in case
your computer crashes when running the detection on a large set of images.

Regardless of your preferred method, you should now double-check that everything
is setup correctly and that your terminal points to the correct directory (i.e.
that you're working within the *Cameratrapping* folder and that you are using
the *cameratrapping* conda environment). Alright, if this is done, let's run
some detections.

## Approach I
In this first approach, we use the *run_tf_detector.py* script and run it from
our computer terminal. The script can either be applied to a single image or to
an entire directory, meaning that it will go through each image in the provided
directory. Let's first take a look at the anatomy of the commands that we'll be
using. The commands to invoke the python script are the following (you'll need
to fill in *filename* and *directoryname* by yourself):

```{bash, eval = FALSE}
# If you want to run the detection on a single image
python run_tf_detector.py md_v4.1.0.pb --input_file filename --output_dir directoryname

# If you want to run the detection on an entire directory
python run_tf_detector.py md_v4.1.0.pb --input_dir directoryname --output_dir directoryname
```

Let me briefly explain what each element of these commands are doing:

* `python run_tf_detector.py` tells our computer to run the run_tf_detector.py
  script using python.

* `md_v4.1.0.pb` is our model file (i.e. the pre-trained neural
  network).

* `--input_file filename` allows us to specify the image on which we want to
  detect images

* `--input_dir directoryname` allows us to specify the directory containing all
  images on which we want to detect animals

* `--output_dir directoryname` allows us to specify the directory into which the
  newly created images should be stored

Of course there are many other options that you may want to play with. However,
this goes beyond the scope of this introduction. To get the full list of
available options, simply type:

```{bash, eval = FALSE}
# Show available options
python run_tf_detector.py -h
```

Anyways, let's run the detector on some real images. In case you have downloaded
my example images, the commands from above translate into:

```{bash, eval = FALSE}
# Run MegaDetector on a single image
python run_tf_detector.py md_v4.1.0.pb --image_file sample_images/L002_R01_20210314013742.JPG --output_dir sample_images/detections

# Run MegaDetector on an entire directory
python run_tf_detector.py md_v4.1.0.pb --image_dir sample_images --output_dir sample_images/detections
```

If everything is setup correctly, the *MegaDetector* will get to work and run
the detections for you. Once the algorithm terminates, you can go to the
*sample_images/detections* folder where you should find the final images with
bounding boxes around each detected animal/human/vehicle. The images should look
something like shown in Figure 3b).

```{r, echo=F, messages=F, out.width="100%", fig.cap='a) Input image and b) output image produced by the run_tf_detector.py script.'}
knitr::include_graphics("/img/gallery_megadetector/figure2.png", error = F)
```

Nice! We just ran our first detection. Now let's see how we can expand this to a
more extensive dataset.

## Approach II
In this approach, we use the *run_tf_batch_detector.py* script. While this
script does not generate neat looking pictures with bounding boxes, it creates a
*.json* file that contains all the detections in a structured format. This
allows us to import the file into R for further analysis. Again, let's first
look at the anatomy of the invoked command. To invoke the batch-detector script,
we can use the following code (you'll need to fill in *filename/directory* and
*filename.json* by yourself):

```{bash, eval = FALSE}
# To run the MegaDetector batch file
python run_tf_detector_batch.py md_v4.1.0.pb --input_file filename/directory --output_file filename.json
```

Again, we want to know what each piece of the code is doing.

* `python run_tf_detector_batch.py` tells our computer to run the
  run_tf_detector_batch.py script using python.

* `md_v4.1.0.pb` is the *MegaDetector* model file (i.e. the pre-trained neural
  network).

* `--input_file filename/directory` allows us to specify the image or a
  directory for which we'd like to run the algorithm

* `--output_file filename.json` allows us to specify the directory into which
  the .json file shoud be stored

Pretty simple ey? For our sample images, this code snippet translates into:

```{bash, eval = FALSE}
# Run the MegaDetector and create a .json file
python run_tf_detector_batch.py md_v4.1.0.pb /home/david/Schreibtisch/CameraTrapping/sample_images /home/david/Schreibtisch/CameraTrapping/sample_images/detections/detections.json
```

Again, the MegaDetector should get to work and start detecting animals.
Depending on your computer and the number of images, this will take a while to
complete, but luckily the MegaDetector gives you detailed information about its
progress while its running.

# Processing the MegaDetector Output
In case you run the *run_tf_detector_batch.py*, you'll end up with a .json file
containing all detections. You can of course open the file in a text-editor and
try to go through the detections by hand. However, I strongly recommend to
further process the file. Here, we will use R to import and clean the file and
to visualize some of the detections. To load the file, we can use the
`fromJSON()` function from the `rjson` package.

```{r, warning=F, message=F}
# Load required packages
library(rjson)      # To read a json file
library(tidyverse)  # For data wrangling
library(exifr)      # To read image meta-data
library(magick)     # To plot images
library(lubridate)  # To handle timestamps

# Set working directory (this will be different for you)
setwd("/home/david/Schreibtisch/CameraTrapping/sample_images")

# Load json file produced by the MegaDetector
anot <- fromJSON(file = "detections/detections.json")

```

Once the .json-file has been loaded, let's take a look at its' structure.

```{r, warning=F, message=F}
# Look at the structure of the json file
str(anot, 2)
```

The file is basically a list of lists containing lots of information on the
original input images, detected categories, and on the algorithm itself. We can
consolidate all of this information into a single tibble/dataframe that is
easier to read:

```{r, warning=F, message=F}

# Extract detected categories and put them into a tibble/dataframe
cats              <- enframe(anot$detection_categories)
names(cats)       <- c("CategoryID", "CategoryName")
cats$CategoryName <- unlist(cats$CategoryName)

# Get information into nice format
info <- tibble(
      images     = map_chr(anot$images, "file")
    , detections = lapply(anot$images, function(x){
      tibble(
          category   = map_chr(x$detections, "category")
        , confidence = map_dbl(x$detections, "conf")
        , bbox       = map(x$detections, "bbox")
      )
    })
  ) %>%
  unnest(detections) %>%
  mutate(bbox = map(bbox, function(x){paste0(x, collapse = ", ")})) %>%
  separate(bbox
    , sep     = ","
    , into    = c("x_left", "y_top", "x_width", "y_height")
    , convert = T
  )

# Join categories to the info table
info <- left_join(info, cats, by = c("category" = "CategoryID"))

# Look at the generated dataframe
head(info)
```

Awesome! Take a look at the output from the cleaned dataframe. It contains a row
for each detection, so that if one image contains more than one detection, there
will be multiple rows for the same image. By looking at the columns, we see that
the file contains information on the detected categories (*cateogry* &
*CategoryName*) and on the bounding boxes surrounding each of the detections.
The bounding boxes are defined by an x and y coordinate (*x_left* & *y_top*) and
a width and a height (*x_width* & *y_height*). Note that the respective numbers
are given on a scale from 0 to 1, so that we'll need to adjust the values
depending on the image resolution. It is also important to note that in computer
vision the y axis starts at the top and not from the bottom. Besides information
on the detected category and the associated bounding box, we also see that for
each detection the detector returned a measure of confidence (*confidence*).
This is really useful because it allows us to subset to detections that exceed a
desired threshold!

Now that we compiled all information into a nice table, we can conduct a wide
range of exploratory analyses. For instance, we may want to identify all images
on which the detector identified an animal with a relatively high confidence. We
can easily subset to those images as follows:

```{r}
# Identify images that contain animals
info %>%
  subset(CategoryName == "animal" & confidence > 0.9)

```

By default, the MegaDetector also returns detections for which he is very
uncertain, which you can see by plotting a histogram of the "Confidence" values.

```{r, out.width = "50%", fig.caption = "Histogram of the confience with which the MegaDetector identified categories in our sample images."}
# Histogram of MegaDetectors confidence
hist(info$confidence
  , breaks = 10
  , main   = "Histogram of Confidence"
  , xlab   = "Confidence"
  , ylab   = "Frequency"
  , col    = "cornflowerblue"
  , border = NA
)

```

For now, we only want to consider detections with relatively high confidence. So
let's subset the data accordingly.

```{r}
# Subset to detections with high confidence
info <- info %>%
  subset(confidence > 0.9)

```

You may also be interested during which time of the day you detected most
animals. For this, we have to extract the timestamp of each cameratrap image. We
can use the function `read_exif()` from the `exifr` package to extract such
information from the image metadata. In fact, the function returns and entire
dataframe of really useful information on each image that we may want to store
for later.

```{r}
# Load image metadata
meta <- read_exif(info$images)

# This contains a ton of information!
names(meta)

# Extract timestamp and add it to our "info" table
info$Timestamp      <- as.POSIXct(meta$CreateDate, format = "%Y:%m:%d %H:%M:%S")
info$TimestampRound <- round(info$Timestamp, "hour")

# Identify number of detections depending on the time of the day
info %>%
  subset(CategoryName == "animal") %>%
  mutate(TimeOfDay = ifelse(hour(TimestampRound) %in% 6:22, "Day", "Night")) %>%
  count(TimeOfDay)

```

But back to the question of when we detected most animals. Here, it seems that
only one of the eight detected animals appeared during the day. This is not
really surprising, as I have rarely seen other animals than our cats during the
day. Despite the useful information shown in the generated dataframe, you will
often also want to visualize the detections. For this, we'll need to load the
respective image into R, draw a bounding box around the detected object and plot
everything. We can write a function that allows us to automate these tasks:

```{r, warning=F, message=F, out.width="100%", fig.cap="Some example detections on cameratrap images. Differently colored bounding boxes represent the different categories. The MegaDetector is able to distinguish animals, humans (person), and vehicles. As you can see on the image on the lower right, the detector even detects animals that are not fully within the picture!"}
# Function to plot the i-th image with bounding box around the detection
showDetection <- function(i = NULL){

  # Load ith image
  img  <- image_read(info$images[i])
  dims <- image_info(img)

  # Plot
  img <- image_draw(img)

  # Add bounding box around the detection (color depending on detected category)
  rect(
      xleft   = info$x_left[i] * dims$width
    , xright  = (info$x_left[i] + info$x_width[i]) * dims$width
    , ytop    = info$y_top[i] * dims$height
    , ybottom = (info$y_top[i] + info$y_height[i]) * dims$height
    , border  = c("red", "yellow", "green")[as.numeric(info$category[i])]
    , lwd     = 10
  )

  # Add a text label indicating the detected class (note that we ensure that the
  # label does not leave the image border on the y-axis).
  text(
      x      = info$x_left[i] * dims$width
    , y      = max(info$y_top[i], 0.05) * dims$height
    , labels = paste(info$CategoryName[i], info$confidence[i])
    , cex    = 5
    , col    = c("red", "yellow", "green")[as.numeric(info$category[i])]
    , adj    = c(0, 0)
  )

  # Add text indicating the image name
  text(
      0.5 * dims$width
    , 0.95 * dims$height
    , basename(info$images[i])
    , cex = 5
    , col = "white"
    , pos = 3
  )
  dev.off()
  plot(img)

}

# Let's try it on some example images
par(mfrow = c(3, 2), mar = c(0, 0, 0, 0))
plots <- lapply(1:6, function(i){showDetection(i)})

```

Cool. It seems like the MegaDetector did a really nice job on our example
images! Obviously the *MegaDetector* is not perfect and there will always be
some misclassifications. Nevertheless, I am often very impressed by the
MegaDetector's ability to identify animals, even on low-light black and white
images. It has happened to me multiple times that I skimmed through images and
didn't realize that an animal was lurking somewhere in the background, yet the
MegaDetector picked it up perfectly! Nevertheless, if you're using the
MegaDetector for scientific purposes, definitely validate its output against
some ground-truthed data ;)

# Summary
Let's summarize what we have learned in this blog post. We have explored how we
can use the *MegaDetector* to easily identify animals on cameratrap images. We
have also learned how we can import the .json output file into R to produce
neat-looking images with bounding boxes around each detected animal. Overall,
the *MegaDetector* is a very powerful and fun tool that allows us to
automatically detect animals on cameratrap images.

# Session Information
```{r, echo=F}
sessionInfo()
```
